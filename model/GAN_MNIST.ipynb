{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6P8AhvPzWaF"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "import torchvision\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzfrj1u90tit"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters etc.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lr = 2e-4\n",
        "batch_size = 64\n",
        "img_dim =  1*28*28  # 784\n",
        "epochs = 100\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "latent_dim=100\n",
        "img_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBj6MIfOzqar"
      },
      "outputs": [],
      "source": [
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    #image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu().view(-1,*size) #.view(-1,size) is added\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "#def make_grad_hook():\n",
        "    '''\n",
        "    Function to keep track of gradients for visualization purposes, \n",
        "    which fills the grads list when using model.apply(grad_hook).\n",
        "    '''\n",
        "    #grads = []\n",
        "    #def grad_hook(m):\n",
        "        #if isinstance(m, nn.Linear) :\n",
        "            #grads.append(m.weight.grad)\n",
        "    #return grads, grad_hook\n",
        "\n",
        "\n",
        "    #def show_tensor_images(image_tensor,num_images=25,size=(1,28,28)):\n",
        "    #image_unflat=image_tensor.detach().cpu().view(-1,*size)\n",
        "    #image_grid=make_grid(image_unflat[:num_images],nrow=5)\n",
        "    #plt.imshow(image_grid.permute(1,2,0).squeeze())\n",
        "    #plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bryofQCf0llX"
      },
      "outputs": [],
      "source": [
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"../../data/mnist\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [ transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzcHRg2HgN9-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3Z6dr1r0ro3"
      },
      "outputs": [],
      "source": [
        "class GAN_generator(nn.Module):\n",
        "     def __init__(self, latent_dim, img_dim):  \n",
        "         super(GAN_generator, self).__init__()\n",
        "\n",
        "         self.net = nn.Sequential(\n",
        "              nn.Linear(latent_dim, 128),\n",
        "              nn.LeakyReLU(),\n",
        "              nn.Linear(128,256),\n",
        "              nn.LeakyReLU(),\n",
        "              nn.Linear(256,512),\n",
        "              nn.LeakyReLU(),\n",
        "              nn.Linear(512,img_dim),\n",
        "              nn.Tanh()\n",
        "              ) \n",
        "         \n",
        "\n",
        "     def forward(self,input_tensor):\n",
        "        return self.net(input_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oQFSVynDRBn"
      },
      "outputs": [],
      "source": [
        "gen = GAN_generator(latent_dim, img_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NH3PiYuCwt6",
        "outputId": "4cd3ed46-5657-470d-ba4e-99143f771771"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GAN_generator(\n",
              "  (net): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.01)\n",
              "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
              "    (3): LeakyReLU(negative_slope=0.01)\n",
              "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (5): LeakyReLU(negative_slope=0.01)\n",
              "    (6): Linear(in_features=512, out_features=784, bias=True)\n",
              "    (7): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 37,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGJF6SwKBMAB"
      },
      "outputs": [],
      "source": [
        "class GAN_discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "         super(GAN_discriminator, self).__init__()\n",
        "\n",
        "         self.disc = nn.Sequential(\n",
        "              nn.Linear(input_dim, 512),\n",
        "              nn.LeakyReLU(),\n",
        "              nn.Linear(512,256),\n",
        "              nn.LeakyReLU(),\n",
        "              nn.Linear(256,128),\n",
        "              nn.LeakyReLU(),\n",
        "              nn.Linear(128,1),\n",
        "              nn.Sigmoid(),\n",
        "              )\n",
        "         \n",
        "\n",
        "    def forward(self,input_tensor):\n",
        "      return self.disc(input_tensor)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S53MiEEkCeL8"
      },
      "outputs": [],
      "source": [
        "dis = GAN_discriminator(img_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqDoOBfeEccN",
        "outputId": "8b670fff-2995-42b2-f966-255b035f4685"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GAN_discriminator(\n",
              "  (disc): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.01)\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): LeakyReLU(negative_slope=0.01)\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): LeakyReLU(negative_slope=0.01)\n",
              "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VqRpdp9Elvm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Loss function\n",
        "loss_function = torch.nn.BCELoss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(gen.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(dis.parameters(), lr=lr, betas=(b1, b2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnl1zZy23LSn"
      },
      "outputs": [],
      "source": [
        "for batch_idx, (real, _) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        #print(real.shape)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        #print(cur_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj8q12yriFCm"
      },
      "outputs": [],
      "source": [
        "def get_noise(n_samples,latent_dim,device=\"cuda\"):\n",
        "    return torch.randn(n_samples,latent_dim,device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwv5vB9ygRpa"
      },
      "outputs": [],
      "source": [
        "def get_disc_loss(generator,discriminator,criterion,real ,num_images,latent_dim,device):\n",
        "    noise=get_noise(num_images,latent_dim,device=device)\n",
        "    gen_out=generator(noise)\n",
        "    disc_fake_out=discriminator(gen_out.detach())\n",
        "    fake_loss=criterion(disc_fake_out,torch.zeros_like(disc_fake_out))\n",
        "    disc_real_out=discriminator(real)\n",
        "    real_loss=criterion(disc_real_out,torch.ones_like(disc_real_out))\n",
        "    disc_loss=(fake_loss+real_loss)/2\n",
        "    return(disc_loss)\n",
        "\n",
        "def get_gen_loss(generator,discriminator,criterion,num_images,latent_dim,device):\n",
        "    noise=get_noise(num_images,latent_dim,device=device)\n",
        "    gen_out=generator(noise)\n",
        "    disc_out=discriminator(gen_out)\n",
        "    loss=criterion(disc_out,torch.ones_like(disc_out))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7ss73fmSFHPe",
        "outputId": "2aa8eb1f-c3cc-4334-9f8b-b3ffc969e5b8"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "\n",
        "# for tensorboard plotting\n",
        "#fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/GAN_MNIST/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/GAN_MNIST/fake\")\n",
        "step = 0\n",
        "display_step = 50 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Going into training mode \n",
        "gen.train()\n",
        "dis.train()\n",
        "\n",
        "generator_losses = []\n",
        "discriminator_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    for batch_idx, (real, _) in enumerate(dataloader):\n",
        "       \n",
        "        cur_batch_size = real.shape[0]\n",
        "        real = real.view(cur_batch_size, -1).to(device)\n",
        "\n",
        "        # Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "        #noise = torch.randn(cur_batch_size, latent_dim).to(device)\n",
        "        #fake = gen(noise)\n",
        "\n",
        "        #disc_real = dis(real).view(-1)\n",
        "        #lossD_real = loss_function(disc_real, torch.ones_like(disc_real))\n",
        "\n",
        "        #disc_fake = dis(fake).view(-1)\n",
        "        #lossD_fake = loss_function(disc_fake, torch.zeros_like(disc_fake))\n",
        "\n",
        "        #loss_dis = (lossD_real + lossD_fake) / 2\n",
        "\n",
        "        #dis.zero_grad()\n",
        "        #loss_dis.backward(retain_graph=True)\n",
        "        #optimizer_D.step()\n",
        "        optimizer_D.zero_grad()\n",
        "        disc_loss=get_disc_loss(gen,dis,loss_function,real,cur_batch_size,latent_dim,device)\n",
        "        disc_loss.backward(retain_graph=True)\n",
        "        optimizer_D.step()\n",
        "\n",
        "        discriminator_losses +=[disc_loss.item()]\n",
        "\n",
        "\n",
        "\n",
        "        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        gen_loss=get_gen_loss(gen,dis,loss_function,cur_batch_size,latent_dim,device)\n",
        "        gen_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        \n",
        "\n",
        "       \n",
        "        \n",
        "        #gen_fake = dis(fake).view(-1)\n",
        "        #loss_gen = loss_function(gen_fake, torch.ones_like(gen_fake))\n",
        "\n",
        "        #gen.zero_grad()\n",
        "        #loss_gen.backward()\n",
        "        #optimizer_G.step()\n",
        "\n",
        "        generator_losses +=[gen_loss.item()]\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
        "                  % (epoch, epochs, batch_idx, len(dataloader),\n",
        "                     disc_loss.item(), gen_loss.item()) )\n",
        "\n",
        "            fake_noise = get_noise(cur_batch_size, latent_dim, device=device)\n",
        "            fake = gen(fake_noise)\n",
        "            show_tensor_images(fake)\n",
        "            show_tensor_images(real)\n",
        "\n",
        "            step_bins = 20\n",
        "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
        "\n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Generator Loss\")\n",
        "            \n",
        "            plt.plot(\n",
        "                range(num_examples // step_bins), \n",
        "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
        "                label=\"Discriminator Loss\")\n",
        "            \n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "  \n",
        "\n",
        "           \n",
        "\n",
        "      \n",
        "        step += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikizNANpr4Z-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}